{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずはライブラリをインポートする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にデータを生成し，適当に欠損させる．説明変数は`x1`と`x2`であり，目的変数は`y`とする．それぞれは\n",
    "$$\n",
    "x_1 \\sim \\mathcal{N}(2.0, 0.4) \\\\\n",
    "x_2 \\mid x_1 \\sim \\mathcal{N}(-2.0 x_1 + 0.5, 0.8) \\\\\n",
    "y \\mid x_1,x_2 \\sim \\mathcal{N}(3.0 x_1 + 1.5x_2, 0.8)\n",
    "$$\n",
    "という関係にあるものとする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x234db166730>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXxUlEQVR4nO3de5RW9X3v8fcngM6xkqAwGGGggzrmiGJjM1iPSVqVpNiDR2JPPBIvTKpZrLhsLufERAxJsCvhLJp0pTEn5xJWtAzRiiS1laWt0ZCmnmaFm7cKopUj1IwgjNgIXhCB7/nj2diH8YHnsp/LzG8+r3+eZ//27bsRP/PjN3v/tiICMzNLy7taXYCZmdWfw93MLEEOdzOzBDnczcwS5HA3M0uQw93MLEEOdxv0JC2V9A1JH5b0TB2P+3eSerLvn5T0j3U89lWSHqzX8cyqNbLVBZhVKiL+L/C+cttJugU4LSKuLnO8P6hHXZI6gS3AqIjYnx37TuDOehzfrBbuuduwowL/3bek+S+4DTqSzpH0qKQ9ku4G2rL2CyT1FW13k6QXsu2ekTRD0sXAl4ErJL0q6Yls259LWiTpF8DrwClZ26cOP7X+h6RXJD0taUbRiq2SPlK0fIukO7LFh7PPX2fn/A8Dh3kknS9pXXbsdZLOL1r3c0lfl/SL7FoelDQuW9cm6Q5JuyT9Otv3pPr8SVvKHO42qEg6Bvgb4IfAicCPgP9cYrv3AX8MTI+I0cBMYGtEPAD8d+DuiDg+In6raLdrgHnAaOBfSpz+d4DngHHAQuAeSSdWUPbvZp9jsnP+ckCtJwL3A98FxgLfBu6XNLZosyuBPwLGA8cAN2btPcB7gEnZvp8G3qigJhvmHO422JwHjAK+ExFvRcSPgXUltjsAHAtMlTQqIrZGxP8rc+ylEbExIvZHxFsl1u8sOu/dwDPArBzXcsgs4NmI+GF27ruAp4H/VLTNX0TEP0fEG8AK4P1Z+1sUQv20iDgQEY9ExO461GSJc7jbYDMBeCEOn9HuHb3siNgMfB64BdgpabmkCWWO/asy60udt9wxKzGBd17DvwATi5ZfLPr+OnB89v2HwE+A5ZK2SfqmpFF1qMkS53C3wWY7MFGSitoml9owIv4yIj4E/CYQwJ8eWnWEY5ebArXUebdl318Djita994qjrstq7HYZOCFMvuR/SviTyJiKnA+cAkwt9x+Zg53G2x+CewHPitppKQ/BM4duJGk90m6SNKxwF4K49AHstU7gM4a7ogZn513lKTLgTOAv83WPQ7MydZ1Ax8v2q8fOAiccoTj/i1wuqQrs2u6ApgK3FeuIEkXSpomaQSwm8IwzYEyu5k53G1wiYh9wB8CnwT+FbgCuKfEpscCi4GXKAxpjKdwlwwUfgkLsEvSo1Wcfg3QlR1zEfDxiNiVrfsqcGpW058Af1lU8+vZ9r/I7mg5b8A17aLQ4/4CsAv4EnBJRLxUQU3vBX5MIdg3Af8A3HHUPcwA+WUdZmbpcc/dzCxBDnczswSVDXdJt0vaKWnDgPbPZE8FbpT0zaL2myVtztbNbETRZmZ2dJVMHLYU+B6w7FCDpAuB2cDZEfGmpPFZ+1RgDnAmhXt7fyrp9Ijwb/fNzJqobLhHxMPZrHfFrgcWR8Sb2TY7s/bZwPKsfYukzRRuY/slRzFu3Ljo7Bx4CjMzO5pHHnnkpYhoL7Wu1il/Twc+LGkRhXuMb4yIdRSeuFtdtF0fhz+F9zZJ8yjM88HkyZNZv359jaWYmQ1PkkrNkQTU/gvVkcAJFOYB+SKwInuyTyW2LXmvZUQsiYjuiOhuby/5g8fMzGpUa7j3AfdEwVoKT+eNy9onFW3Xwb89vm1mZk1Sa7j/DXARgKTTKUxR+hKwksIj2sdKmkLhab+1dajTzMyqUHbMXdJdwAXAuOxFCQuB24Hbs9sj9wE92Wx6GyWtAJ6iMD/IDb5TxswGg7feeou+vj727t3b6lKq1tbWRkdHB6NGVT4h6KCYfqC7uzv8C1Uza6QtW7YwevRoxo4dy+GTfw5uEcGuXbvYs2cPU6ZMOWydpEciorvUfn5C1cyGhb179w65YAeQxNixY6v+F4fD3cyGjaEW7IfUUrfD3cwsQbU+xGRmNqR1zr+/rsfburi21+1efPHFrF69mg996EPcd1/Z97dUzOFuR3fLeyrY5pXG12GWqC9+8Yu8/vrrfP/736/rcT0sY2bWBOvWrePss89m7969vPbaa5x55pls2LCBGTNmMHr06Lqfzz13M7MmmD59Opdeeilf+cpXeOONN7j66qs566yzGnY+h7uZWZN87WtfY/r06bS1tfHd7363oefysIyZWZO8/PLLvPrqq+zZs6fhT8o63M3MmmTevHl8/etf56qrruKmm25q6Lk8LGNmw1Ktty7WatmyZYwcOZIrr7ySAwcOcP755/Ozn/2MhQsX8vTTT/Pqq6/S0dHBbbfdxsyZ+d9Q6nA3M2uCuXPnMnfuXABGjBjBmjVrALjooosacj4Py5iZJcg9dzOrm+KnPps97GGHc8/dzCxBDnczswQ53M3MEuRwNzNLUCXvUL0duATYGRFnDVh3I/AtoD0iXsrabgauAw4An42In9S9ajOzvCqZ8bSq41U/O+rjjz/O9ddfz+7duxkxYgQLFizgiiuuqEs5ldwtsxT4HrCsuFHSJOCjwPNFbVOBOcCZwATgp5JO90uyzcze6bjjjmPZsmV0dXWxbds2PvCBDzBz5kzGjBmT+9hlh2Ui4mHg5RKr/hz4ElD8hu3ZwPKIeDMitgCbgXNzV2lmNsSVmvJ33759dHV1ATBhwgTGjx9Pf39/Xc5X033uki4FXoiIJwa8228isLpouS9rK3WMecA8gMmTJ9dShpnZkFFuyt+1a9eyb98+Tj311Lqcr+pwl3QcsAD4/VKrS7RFiTYiYgmwBKC7u7vkNmZmKTnSlL/bt2/nmmuuobe3l3e9qz73udRylFOBKcATkrYCHcCjkt5Loac+qWjbDmBb3iLNzFJQasrf3bt3M2vWLL7xjW9w3nnn1e1cVYd7RDwZEeMjojMiOikE+m9HxIvASmCOpGMlTQG6gLV1q9bMbAgbOOXvvn37uOyyy5g7dy6XX355Xc9Vya2QdwEXAOMk9QELI+K2UttGxEZJK4CngP3ADb5TxswGpSa/2L3UlL/Lly/n4YcfZteuXSxduhSApUuX8v73vz/3+cqGe0R8osz6zgHLi4BF+coyM0vLkab8PdRWb35C1cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7ME+R2qZjYsTeudVtfjPdnzZF2Pl5d77mZmCXK4m5k1wVe/+lVuvfXWt5cXLFhw2ORh9eZwNzNrguuuu47e3l4ADh48yPLly7nqqqsadj6PuZuZNUFnZydjx47lscceY8eOHZxzzjmMHTu2YedzuJuZNcmnPvUpli5dyosvvsi1117b0HN5WMbMrEkuu+wyHnjgAdatW8fMmTMbei733C2/St4i3+TpVc3KacWti8cccwwXXnghY8aMYcSIEQ09l8PdzKxJDh48yOrVq/nRj37U8HN5WMbMrAmeeuopTjvtNGbMmEFXV1fDz+eeu5lZE0ydOpXnnnuuaedzuA9nlYyVmyUkIpDU6jKqFhFV71N2WEbS7ZJ2StpQ1PYtSU9L+idJfy1pTNG6myVtlvSMpMb+OtjMrEJtbW3s2rWrpqBspYhg165dtLW1VbVfJT33pcD3gGVFbQ8BN0fEfkl/CtwM3CRpKjAHOBOYAPxU0ul+SbaZtVpHRwd9fX309/e3upSqtbW10dHRUdU+lbwg+2FJnQPaHixaXA18PPs+G1geEW8CWyRtBs4FfllVVWZmdTZq1CimTJnS6jKaph5j7tcCd2ffJ1II+0P6srZ3kDQPmAcwefLkOpRhNrxUO2XtYJuS1hor162QkhYA+4E7DzWV2KzkAFdELImI7ojobm9vz1OGmZkNUHPPXVIPcAkwI/7tNxR9wKSizTqAbbWXZ2b1Uk1P3738oa+mnruki4GbgEsj4vWiVSuBOZKOlTQF6ALW5i/TzMyqUbbnLuku4AJgnKQ+YCGFu2OOBR7K7hldHRGfjoiNklYAT1EYrrnBd8qYVa7er36rlXv5Q18ld8t8okTzbUfZfhGwKE9RZmaWj+eWMTNLkMPdzCxBDnczswQ53M3MEuRwNzNLkKf8TZmn9DUbttxzNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBvhXSmqPcbZm3vNKcOqxpOuff//b3rYtntbCS4ck9dzOzBDnczcwS5GEZswYaLC/fsOHH4W5muRT/ABt9xpG2mf/2d7+5qTk8LGNmlqCy4S7pdkk7JW0oajtR0kOSns0+Tyhad7OkzZKekTSzUYWbmdmRVdJzXwpcPKBtPrAqIrqAVdkykqYCc4Azs33+l6QRdavWzMwqUjbcI+Jh4OUBzbOB3ux7L/CxovblEfFmRGwBNgPn1qdUMzOrVK1j7idFxHaA7HN81j4R+FXRdn1Z2ztImidpvaT1/f39NZZhZmal1PsXqirRFqU2jIglEdEdEd3t7e11LsPMbHirNdx3SDoZIPvcmbX3AZOKtusAttVenpmZ1aLWcF8J9GTfe4B7i9rnSDpW0hSgC1ibr0QzM6tW2YeYJN0FXACMk9QHLAQWAyskXQc8D1wOEBEbJa0AngL2AzdExIEG1W5mZkdQNtwj4hNHWDXjCNsvAhblKcrMzPLxE6pmZglyuJuZJcjhbmaWIIe7mVmCHO5mZgnyfO5DVbl3kprZsOaeu5lZghzuZmYJcribmSXI4W5mliCHu5lZgny3jJk1Vef8+9/+vnXxrBZWkjaHu1mVpvVOa3UJZmV5WMbMLEEOdzOzBDnczcwS5HA3M0uQw93MLEG5wl3Sf5W0UdIGSXdJapN0oqSHJD2bfZ5Qr2LNzKwyNYe7pInAZ4HuiDgLGAHMAeYDqyKiC1iVLZuZWRPlHZYZCfw7SSOB44BtwGygN1vfC3ws5znMzKxKNYd7RLwA/BnwPLAdeCUiHgROiojt2TbbgfH1KNTMzCqXZ1jmBAq99CnABOA3JF1dxf7zJK2XtL6/v7/WMszMrIQ8wzIfAbZERH9EvAXcA5wP7JB0MkD2ubPUzhGxJCK6I6K7vb09RxlmZjZQnnB/HjhP0nGSBMwANgErgZ5smx7g3nwlmplZtWqeOCwi1kj6MfAosB94DFgCHA+skHQdhR8Al9ejUDMzq1yuWSEjYiGwcEDzmxR68WZm1iJ+QtXMLEEOdzOzBDnczcwS5HA3M0uQw93MLEF+h6qZtYxflt047rmbmSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSXIT6iaAdN6p7W6BLO6cs/dzCxBDnczswTlGpaRNAb4AXAWEMC1wDPA3UAnsBX4LxHxr3nOM+zc8p5WV2BmQ1zenvutwAMR8e+B3wI2AfOBVRHRBazKls3MrIlqDndJ7wZ+F7gNICL2RcSvgdlAb7ZZL/CxfCWamVm18vTcTwH6gb+Q9JikH0j6DeCkiNgOkH2Or0OdZmZWhTzhPhL4beB/R8Q5wGtUMQQjaZ6k9ZLW9/f35yjDzMwGyhPufUBfRKzJln9MIex3SDoZIPvcWWrniFgSEd0R0d3e3p6jDDMzG6jmcI+IF4FfSXpf1jQDeApYCfRkbT3AvbkqNDOzquV9QvUzwJ2SjgGeA/6Iwg+MFZKuA54HLs95DjMzq1KucI+Ix4HuEqtm5DmuDUOV3Nt/yyuNr8MsEX5C1cwsQQ53M7MEOdzNzBLkKX/NrKlGn1H6cZhpve9sf7LnyUaXkyz33M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBvs/dhg7PP2NWMYd7s/nl12bWBB6WMTNLkMPdzCxBDnczswQ53M3MEuRwNzNLUO5wlzRC0mOS7suWT5T0kKRns88T8pdpZmbVqMetkJ8DNgHvzpbnA6siYrGk+dnyTXU4j5kNM9N6p1W8red+P1yucJfUAcwCFgH/LWueDVyQfe8Ffo7D3ZqsmlAwS1HeYZnvAF8CDha1nRQR2wGyz/E5z2FmZlWqOdwlXQLsjIhHatx/nqT1ktb39/fXWoaZmZWQp+f+QeBSSVuB5cBFku4Adkg6GSD73Flq54hYEhHdEdHd3t6eowwzMxuo5nCPiJsjoiMiOoE5wM8i4mpgJdCTbdYD3Ju7SjMzq0oj7nNfDHxU0rPAR7NlMzNrorrMChkRP6dwVwwRsQuYUY/jmplZbfyEqplZgjyfu6Xl0Hz5Uya3tg6zFnPP3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5AfYqq3Qw/RmJm1kHvuZmYJcribmSXI4W5mliCHu5lZgvwLVRsypnmmR7OKueduZpYgh7uZWYIc7mZmCao53CVNkvT3kjZJ2ijpc1n7iZIekvRs9nlC/co1M7NK5Om57we+EBFnAOcBN0iaCswHVkVEF7AqWzYzsyaqOdwjYntEPJp93wNsAiYCs4HebLNe4GM5azQzsyrVZcxdUidwDrAGOCkitkPhBwAw/gj7zJO0XtL6/v7+epRhZmaZ3OEu6Xjgr4DPR8TuSveLiCUR0R0R3e3t7XnLMDOzIrkeYpI0ikKw3xkR92TNOySdHBHbJZ0M7MxbpJlZOdN6p1W1/ZM9TzaoksEhz90yAm4DNkXEt4tWrQR6su89wL21l2dmZrXI03P/IHAN8KSkx7O2LwOLgRWSrgOeBy7PVaGZWQNU09Mfir38msM9Iv4R0BFWz6j1uGZmlp+fUDUzS5DD3cwsQQ53M7MEeT73avjl12Y2RLjnbmaWIIe7mVmCHO5mZgnymLu1lN+LatYY7rmbmSXI4W5mliCHu5lZgjzmfojvYTezhLjnbmaWIPfczczKGIovAnHP3cwsQWn03MuNl9/ySnPqMN+3bsbgeBGIe+5mZglKo+deju+EycW9cbOhp2HhLuli4FZgBPCDiFjcqHNZdRzWZulryLCMpBHA/wT+AJgKfELS1Eacy8zM3qlRY+7nApsj4rmI2AcsB2Y36FxmZjZAo4ZlJgK/KlruA36neANJ84B52eKrkp5pUC31Ng54qdVF5LMBkriOt6VyLb6OwaUp16FPKs/uv3mkFY0K91LVxmELEUuAJQ06f8NIWh8R3a2uI69UrgPSuRZfx+Ay1K+jUcMyfcCkouUOYFuDzmVmZgM0KtzXAV2Spkg6BpgDrGzQuczMbICGDMtExH5Jfwz8hMKtkLdHxMZGnKsFhtxQ0hGkch2QzrX4OgaXIX0diojyW5mZ2ZDi6QfMzBLkcDczS5DDvUaSPiPpGUkbJX2z1fXkIelGSSFpXKtrqYWkb0l6WtI/SfprSWNaXVM1JF2c/V3aLGl+q+uplaRJkv5e0qbs/4vPtbqmPCSNkPSYpPtaXUstHO41kHQhhSduz46IM4E/a3FJNZM0Cfgo8Hyra8nhIeCsiDgb+Gfg5hbXU7HEpurYD3whIs4AzgNuGMLXAvA5YFOri6iVw7021wOLI+JNgIjY2eJ68vhz4EsMeMhsKImIByNif7a4msJzFUNFMlN1RMT2iHg0+76HQjBObG1VtZHUAcwCftDqWmrlcK/N6cCHJa2R9A+Spre6oFpIuhR4ISKeaHUtdXQt8HetLqIKpabqGJKBWExSJ3AOsKbFpdTqOxQ6PQdbXEfNhsd87jWQ9FPgvSVWLaDw53YChX96TgdWSDolBuF9pWWu48vA7ze3otoc7Toi4t5smwUUhgbubGZtOZWdqmOokXQ88FfA5yNid6vrqZakS4CdEfGIpAtaXE7NHO5HEBEfOdI6SdcD92RhvlbSQQqTDPU3q75KHek6JE0DpgBPSILCUMajks6NiBebWGJFjvbfA0BSD3AJMGMw/pA9iqSm6pA0ikKw3xkR97S6nhp9ELhU0n8E2oB3S7ojIq5ucV1V8UNMNZD0aWBCRHxN0unAKmDyEAuVw0jaCnRHxJCbzS97Mcy3gd+LiEH3A/ZoJI2k8EvgGcALFKbuuHIoPtGtQi+hF3g5Ij7f4nLqIuu53xgRl7S4lKp5zL02twOnSNpA4RdgPUM52BPwPWA08JCkxyX9n1YXVKnsF8GHpurYBKwYisGe+SBwDXBR9t/h8az3ay3gnruZWYLcczczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7ME/X/EXbQX34mi5AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_train, N_test = 60, 1000\n",
    "X = np.zeros((N_train + N_test, 2))\n",
    "y = np.zeros(N_train + N_test)\n",
    "X[:,0] = np.random.normal(2.0, 0.4, size = N_train + N_test)\n",
    "X[:,1] = np.random.normal(-2.0*X[:,0] + 0.5, 0.8, size = N_train + N_test)\n",
    "y = np.random.normal(3.0*X[:,0] + 1.5*X[:,1], 0.8, size = N_train + N_test)\n",
    "X_train_complete = X[:N_train]\n",
    "y_train = y[:N_train]\n",
    "X_test = X[N_train:]\n",
    "y_test = y[N_train:]\n",
    "\n",
    "plt.hist(X[:,0], bins = 20, label = \"x1\")\n",
    "plt.hist(X[:,1], bins = 20, label = \"x2\")\n",
    "plt.hist(y, bins = 20, label = \"y\")\n",
    "plt.title(\"distributions\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p = 0.3$の確率で説明変数を欠損させる．欠損した位置や値は`table_missing`で保存する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.58535427         nan]\n",
      " [ 2.10601671 -4.55318535]\n",
      " [ 1.80926526 -3.5396434 ]\n",
      " [ 2.38851039 -4.33333887]\n",
      " [ 1.42494009         nan]\n",
      " [ 1.4851974  -2.11832497]\n",
      " [ 2.36502598 -3.75242383]\n",
      " [        nan -2.56203429]\n",
      " [ 1.40867681 -2.91245052]\n",
      " [ 2.34858366         nan]]\n"
     ]
    }
   ],
   "source": [
    "p = 0.3\n",
    "X_train = X_train_complete.copy()\n",
    "is_nan_X_train = np.zeros(X_train.shape, dtype = \"int\")\n",
    "for idx in range(len(X_train)):\n",
    "    if np.random.rand() < p:\n",
    "        is_nan_X_train[idx, 0] = 1\n",
    "        X_train[idx, 0] = np.nan\n",
    "    \n",
    "    if np.random.rand() < p:\n",
    "        is_nan_X_train[idx, 1] = 1\n",
    "        X_train[idx, 1] = np.nan\n",
    "print(X_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====最尤推定(complete data)=====\n",
      "y_error = 0.6841894767175021\n",
      "=====最尤推定(dropped)=====\n",
      "y_error = 0.7259293059445424\n",
      "=====最尤推定(interpolated)=====\n",
      "y_error = 0.9199739418129449\n"
     ]
    }
   ],
   "source": [
    "print(\"=====最尤推定(complete data)=====\")\n",
    "w_ML = np.linalg.inv(X_train_complete.T.dot(X_train_complete)).dot(X_train_complete.T).dot(y_train)\n",
    "y_pred = X_test.dot(w_ML)\n",
    "print(\"y_error = {:}\".format( ((y_pred - y_test)**2.0).mean() ))\n",
    "\n",
    "print(\"=====最尤推定(dropped)=====\")\n",
    "X_train_dropped = X_train[is_nan_X_train.sum(1) == 0]\n",
    "y_train_dropped = y_train[is_nan_X_train.sum(1) == 0]\n",
    "w_ML = np.linalg.inv(X_train_dropped.T.dot(X_train_dropped)).dot(X_train_dropped.T).dot(y_train_dropped)\n",
    "y_pred = X_test.dot(w_ML)\n",
    "print(\"y_error = {:}\".format( ((y_pred - y_test)**2.0).mean() ))\n",
    "\n",
    "print(\"=====最尤推定(interpolated)=====\")\n",
    "X_train_interpolated = X_train.copy()\n",
    "X_train_interpolated[np.isnan(X_train)] = np.tile(np.nanmean(X_train, axis = 0), reps = (len(X_train,),1))[np.isnan(X_train)]\n",
    "w_ML = np.linalg.inv(X_train_interpolated.T.dot(X_train_interpolated)).dot(X_train_interpolated.T).dot(y_train)\n",
    "y_pred = X_test.dot(w_ML)\n",
    "print(\"y_error = {:}\".format( ((y_pred - y_test)**2.0).mean() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "説明変数の補間が可能な，ベイズ線形回帰のクラスを構成する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bayesian_LR_with_interpolation: # 説明変数の補間が可能な，ベイズ線形回帰\n",
    "    def __init__(self, alpha = 1.0, beta = 1.0):\n",
    "        self.alpha = alpha # alphaはパラメータwの事前分布のパラメータで，alphaが大きいほどwの分布は狭くなる．\n",
    "        self.beta = beta # betaは誤差のオーダーの逆数\n",
    "    \n",
    "    def fit(self, X_train_original, y_train_original, num_sample = 1000, fit_hyper_parameters = True, num_iteration = 100, eta = 0.1):\n",
    "        dalpha = 0.01\n",
    "        dbeta = 0.01\n",
    "        self.hyper_param_record = np.zeros((num_iteration, 2)) # ハイパーパラメータの軌跡を保存\n",
    "        self.normalize_train(X_train_original, y_train_original) # 標準化されたself.X_train, self.y_trainが生成される\n",
    "        if len(self.nan_list) == 0:\n",
    "            print(\"Note: There are no missing values in the explanatory variables.\")\n",
    "        \n",
    "        w_sample_mean = np.zeros(len(self.X_train[0])) # サンプリングから計算したwの期待値を格納\n",
    "        w_sample_std = np.zeros(len(self.X_train[0])) # サンプリングから計算したwの標準偏差を格納\n",
    "        X_sample_mean = self.X_train.copy() # サンプリングから計算したXの期待値を格納，欠損値だけ更新\n",
    "        X_sample_std = np.zeros(self.X_train.shape) # サンプリングから計算したXの標準偏差を格納，欠損値だけ更新\n",
    "        X_sample = self.X_train.copy() # 現時点でのサンプルの値，欠損値だけ更新する\n",
    "        self.w_samples = np.zeros((num_sample, len(self.X_train[0]))) # wに関するサンプリング結果をすべて集める，予測の際に必要\n",
    "        \n",
    "        # ハイパーパラメータを調整，勾配法により尤度を上げるようなハイパーパラメータを探す\n",
    "        if fit_hyper_parameters: # ハイパーパラメータを調整する場合\n",
    "            likelihood = self.likelihood(num_sample) # 初期化\n",
    "            for idx_iteration in range(num_iteration):\n",
    "                self.hyper_param_record[idx_iteration] = np.array([self.alpha, self.beta])\n",
    "                self.alpha = self.alpha + dalpha\n",
    "                likelihood_new = self.likelihood(num_sample)\n",
    "                self.alpha = self.alpha - dalpha + eta*(likelihood_new - likelihood) / dalpha\n",
    "                likelihood = likelihood_new\n",
    "                self.beta = self.beta + dbeta\n",
    "                likelihood_new = self.likelihood(num_sample) # 新しい尤度を計算\n",
    "                self.beta = self.beta - dbeta + eta*(likelihood_new - likelihood) / dbeta\n",
    "                likelihood = likelihood_new\n",
    "                print(\"iteraton = {0:} / {1:}: alpha = {2:.6f}, beta = {3:.6f}, L = {4:.6f}\".format(idx_iteration, num_iteration, self.alpha, self.beta, likelihood), end = \"\\r\")\n",
    "        \n",
    "        # サンプリングによりwとXの統計量を計算\n",
    "        for t in range(num_sample):\n",
    "            # wのサンプリング\n",
    "            w_sample = self.sample_w(X_sample)\n",
    "            self.w_samples[t] = w_sample # predictionで使うため\n",
    "            w_sample_mean = self.update_mean(w_sample_mean, w_sample, t)\n",
    "            w_sample_std = self.update_std(w_sample_std, w_sample, t)\n",
    "            \n",
    "            # Xのサンプリング(欠損している箇所のみ)\n",
    "            for idx, (n, m) in enumerate(self.nan_list):\n",
    "                X_sample[n, m] = self.sample_x_nm(n, m, w_sample, X_sample[n], self.y_train[n])\n",
    "                X_sample_mean[n, m] = self.update_mean(X_sample_mean[n, m], X_sample[n, m], t)\n",
    "                X_sample_std[n, m] = self.update_std(X_sample_std[n, m], X_sample[n, m], t)\n",
    "                \n",
    "        # 標準化を解除\n",
    "        X_sample_mean = np.tile(self.x_train_original_mean, reps = (len(X_train_original), 1)) \\\n",
    "                            + X_sample_mean * np.tile(self.x_train_original_std, reps = (len(X_train_original), 1))\n",
    "        X_sample_std = np.tile(self.x_train_original_std, reps = (len(X_train_original), 1)) * X_sample_std\n",
    "        print(\"Fitting has finished: \")\n",
    "        print(\"alpha = {:.6f}\".format(self.alpha))\n",
    "        print(\"beta = {:.6f}\".format(self.beta))\n",
    "        return w_sample_mean, w_sample_std, X_sample_mean, X_sample_std\n",
    "    \n",
    "    def normalize_train(self, X_train_original, y_train_original): # 入力を標準化しつつ，標準化を解除できるように値を保存\n",
    "        # Xについての処理\n",
    "        self.x_train_original_mean = np.nanmean(X_train_original, axis = 0)\n",
    "        self.x_train_original_std = np.nanstd(X_train_original, axis = 0)\n",
    "        self.X_train = (X_train_original - np.tile(self.x_train_original_mean, reps = (len(X_train_original),1)) ) \\\n",
    "                            / np.tile(self.x_train_original_std, reps = (len(X_train_original), 1)) # 標準化\n",
    "        self.Lambda = np.linalg.inv( np.cov(self.X_train[np.isnan(X_train_original).sum(1) == 0].T) )# 説明変数がどれも欠損していないデータ行から計算した共分散行列の逆行列\n",
    "        self.nan_list = list(zip(*np.where(np.isnan(X_train_original)))) # もとの入力データでnullの場所を格納\n",
    "        self.X_train[np.isnan(X_train_original)] = 0.0 # 標準化しているため0で埋めてok\n",
    "        \n",
    "        # yについての処理\n",
    "        self.y_train_original_mean = y_train_original.mean()\n",
    "        self.y_train_original_std = y_train_original.std()\n",
    "        self.y_train = (y_train_original - self.y_train_original_mean) / self.y_train_original_std\n",
    "    \n",
    "    def likelihood(self, num_sample):\n",
    "        L = 0.0\n",
    "        X_sample = self.X_train.copy()\n",
    "        for t in range(num_sample):\n",
    "            w_sample = self.sample_w(X_sample)\n",
    "            for idx, (n, m) in enumerate(self.nan_list):\n",
    "                X_sample[n, m] = self.sample_x_nm(n, m, w_sample, X_sample[n], self.y_train[n])\n",
    "            L += np.exp( -0.5 * self.beta * ((self.y_train - X_sample.dot(w_sample))**2.0).sum() )\n",
    "        # 訓練データ数が増えるにつれてlog_likelihoodは線形に増える(はず)なので規格化．これをしないとデータ数を変えるたびにetaを変更する必要がありそう\n",
    "        L = np.log(L) / len(self.y_train) + 0.5*np.log(self.beta) \n",
    "        return L\n",
    "    \n",
    "    def update_mean(self, x_mean, x, t):\n",
    "        return (1.0 / (t+1.0)) * (t*x_mean + x)\n",
    "    \n",
    "    def update_std(self, x_std, x, t):\n",
    "        return np.sqrt( (t/(t+1.0)) * (x_std**2.0 + (x_std-x)**2.0 / (t+1.0) ) )\n",
    "    \n",
    "    def sample_w(self, X): # 他を条件づけたときのwをサンプル\n",
    "        y, alpha, beta = self.y_train, self.alpha, self.beta\n",
    "        w_mu = np.linalg.inv(X.T.dot(X) + (alpha/beta)*np.eye(len(X[0]))).dot(X.T).dot(y) # 他の変数を条件づけたときの分布の期待値n\n",
    "        w_sigma = np.linalg.inv(beta * X.T.dot(X) + alpha*np.eye(len(X[0]))) # 標準偏差\n",
    "        return np.random.multivariate_normal(w_mu, w_sigma)\n",
    "    \n",
    "    def sample_x_nm(self, n, m, w, x_n, y_n): # 他を条件づけたときのx_nm(欠損値)をサンプル\n",
    "        alpha, beta = self.alpha, self.beta\n",
    "        x_nm_mu = x_n[m] + (beta*y_n*w[m] - (beta*w*w[m]+self.Lambda[m]).dot(x_n)) / (beta*w[m]*w[m] + self.Lambda[m,m])\n",
    "        x_nm_sigma = 1.0 / np.sqrt(beta*w[m]*w[m] + self.Lambda[m,m])\n",
    "        return np.random.normal(x_nm_mu, x_nm_sigma)\n",
    "    \n",
    "    def sample_y(self, X, w): # 他を条件づけたときの出力yをサンプル\n",
    "        y_mu = X.dot(w)\n",
    "        y_sigma = (1.0/self.beta) * np.eye(len(y_mu))\n",
    "        return np.random.multivariate_normal(y_mu, y_sigma)\n",
    "    \n",
    "    def predict(self, X_test_original, num_sample = 1000): # 欠損がある可能性のある入力から予測\n",
    "        X_test = (X_test_original - np.tile(self.x_train_original_mean, reps = (len(X_test_original),1)) ) \\\n",
    "                            / np.tile(self.x_train_original_std, reps = (len(X_test_original), 1)) # 標準化\n",
    "        nan_list = list(zip(*np.where(np.isnan(X_test_original)))) # もとの入力データでnullの場所を格納\n",
    "        X_test[np.isnan(X_test_original)] = 0.0 # 標準化しているため0で埋めてok\n",
    "        \n",
    "        X_test_sample_mean = X_test.copy() # サンプリングから計算したXの期待値を格納，欠損値だけ更新\n",
    "        X_test_sample_std = np.zeros(X_test.shape) # サンプリングから計算したXの標準偏差を格納，欠損値だけ更新\n",
    "        X_test_sample = X_test.copy() # 現時点でのサンプルの値，欠損値だけ更新する\n",
    "        \n",
    "        y_test_sample_mean = np.zeros(len(X_test_original))\n",
    "        y_test_sample_std = np.zeros(len(X_test_original))\n",
    "        \n",
    "        # サンプリングによりyとXの統計量を計算\n",
    "        for t in range(num_sample):\n",
    "            print(\"prediction: t = {0:} / {1:}\".format(t, num_sample), end = \"\\r\")\n",
    "            # wのサンプル(前の結果を流用)\n",
    "            w_sample = self.w_samples[t]\n",
    "            # yのサンプル\n",
    "            y_test_sample = self.sample_y(X_test_sample, w_sample)\n",
    "            y_test_sample_mean = self.update_mean(y_test_sample_mean, y_test_sample, t)\n",
    "            y_test_sample_std = self.update_std(y_test_sample_std, y_test_sample, t)\n",
    "            # Xのサンプル           \n",
    "            for idx, (n, m) in enumerate(nan_list):\n",
    "                X_test_sample[n, m] = self.sample_x_nm(n, m, w_sample, X_test_sample[n], y_test_sample[n])\n",
    "                X_test_sample_mean[n, m] = self.update_mean(X_test_sample_mean[n, m], X_test_sample[n, m], t)\n",
    "                X_test_sample_std[n, m] = self.update_std(X_test_sample_std[n, m], X_test_sample[n, m], t)\n",
    "        # 標準化を解除\n",
    "        y_test_sample_mean = self.y_train_original_mean + self.y_train_original_std * y_test_sample_mean\n",
    "        y_test_sample_std = self.y_train_original_std * y_test_sample_std\n",
    "        X_test_sample_mean = np.tile(self.x_train_original_mean, reps = (len(X_test), 1)) \\\n",
    "                            + X_test_sample_mean * np.tile(self.x_train_original_std, reps = (len(X_test), 1))\n",
    "        X_test_sample_std = np.tile(self.x_train_original_std, reps = (len(X_test), 1)) * X_test_sample_std\n",
    "        print(\"Predictions has finished.\")\n",
    "        return y_test_sample_mean, y_test_sample_std, X_test_sample_mean, X_test_sample_std\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠損のあるデータから予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====ベイズ線形回帰=====\n",
      "Fitting has finished: lpha = 2.567708, beta = 4.346280, L = 0.3845116\n",
      "alpha = 2.567708\n",
      "beta = 4.346280\n",
      "X_error = 0.024276115858132833, 0.2436076829842362\n",
      "Predictions has finished.0\n",
      "y_error = 0.7147843731641442\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=====ベイズ線形回帰=====\")\n",
    "model = Bayesian_LR_with_interpolation(alpha = 1.0, beta = 1.0)\n",
    "X_pred = model.fit(X_train, y_train, fit_hyper_parameters = True)[2]\n",
    "x1_error, x2_error = ( (X_pred - X_train_complete)**2.0 ).mean(0)\n",
    "print(\"X_error = {0:}, {1:}\".format(x1_error, x2_error))\n",
    "y_pred = model.predict(X_test)[0]\n",
    "print(\"y_error = {:}\".format( ((y_pred - y_test)**2.0).mean() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠損のないデータから予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====最尤推定=====\n",
      "y_error = 0.6841894767175021\n",
      "\n",
      "=====ベイズ線形回帰=====\n",
      "Note: There are no missing values in the explanatory variables.\n",
      "Fitting has finished: lpha = 1.482402, beta = 2.115292, L = 0.0693715\n",
      "alpha = 1.482402\n",
      "beta = 2.115292\n",
      "X_error = 0.0, 4.108650548026103e-33\n",
      "Predictions has finished.0\n",
      "y_error = 0.6814939439707199\n"
     ]
    }
   ],
   "source": [
    "print(\"=====最尤推定=====\")\n",
    "w_ML = np.linalg.inv(X_train_complete.T.dot(X_train_complete)).dot(X_train_complete.T).dot(y_train)\n",
    "y_pred = X_test.dot(w_ML)\n",
    "print(\"y_error = {:}\".format( ((y_pred - y_test)**2.0).mean() ))\n",
    "\n",
    "print(\"\\n=====ベイズ線形回帰=====\")\n",
    "model = Bayesian_LR_with_interpolation(alpha = 1.0, beta = 1.0)\n",
    "X_pred = model.fit(X_train_complete, y_train, fit_hyper_parameters = True)[2]\n",
    "x1_error, x2_error = ( (X_pred - X_train_complete)**2.0 ).mean(0)\n",
    "print(\"X_error = {0:}, {1:}\".format(x1_error, x2_error))\n",
    "y_pred = model.predict(X_test)[0]\n",
    "print(\"y_error = {:}\".format( ((y_pred - y_test)**2.0).mean() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠損データをすべて落として予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====最尤推定=====\n",
      "y_error = 0.7259293059445424\n",
      "\n",
      "=====ベイズ線形回帰=====\n",
      "Note: There are no missing values in the explanatory variables.\n",
      "Fitting has finished: lpha = 1.721914, beta = 3.476663, L = 0.428672\n",
      "alpha = 1.721914\n",
      "beta = 3.476663\n",
      "Predictions has finished.0\n",
      "y_error = 0.7766452769398361\n"
     ]
    }
   ],
   "source": [
    "X_train_dropped = X_train[is_nan_X_train.sum(1) == 0]\n",
    "y_train_dropped = y_train[is_nan_X_train.sum(1) == 0]\n",
    "\n",
    "print(\"=====最尤推定=====\")\n",
    "w_ML = np.linalg.inv(X_train_dropped.T.dot(X_train_dropped)).dot(X_train_dropped.T).dot(y_train_dropped)\n",
    "y_pred = X_test.dot(w_ML)\n",
    "print(\"y_error = {:}\".format( ((y_pred - y_test)**2.0).mean() ))\n",
    "\n",
    "print(\"\\n=====ベイズ線形回帰=====\")\n",
    "model = Bayesian_LR_with_interpolation(alpha = 2.0, beta = 3.0)\n",
    "model.fit(X_train_dropped, y_train_dropped, fit_hyper_parameters = True)\n",
    "y_pred = model.predict(X_test)[0]\n",
    "print(\"y_error = {:}\".format( ((y_pred - y_test)**2.0).mean() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠損データを平均値で補完して予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====最尤推定=====\n",
      "y_error = 0.9199739418129449\n",
      "\n",
      "=====ベイズ線形回帰=====\n",
      "Note: There are no missing values in the explanatory variables.\n",
      "Fitting has finished: lpha = 4.767952, beta = 1.785241, L = -0.221673\n",
      "alpha = 4.767952\n",
      "beta = 1.785241\n",
      "X_error = 0.03779597748274369, 0.5054058111998435\n",
      "Predictions has finished.0\n",
      "y_error = 0.9434735826603966\n"
     ]
    }
   ],
   "source": [
    "X_train_interpolated = X_train.copy()\n",
    "X_train_interpolated[np.isnan(X_train)] = np.tile(np.nanmean(X_train, axis = 0), reps = (len(X_train,),1))[np.isnan(X_train)]\n",
    "\n",
    "print(\"=====最尤推定=====\")\n",
    "w_ML = np.linalg.inv(X_train_interpolated.T.dot(X_train_interpolated)).dot(X_train_interpolated.T).dot(y_train)\n",
    "y_pred = X_test.dot(w_ML)\n",
    "print(\"y_error = {:}\".format( ((y_pred - y_test)**2.0).mean() ))\n",
    "\n",
    "print(\"\\n=====ベイズ線形回帰=====\")\n",
    "model = Bayesian_LR_with_interpolation(alpha = 2.0, beta = 3.0)\n",
    "X_pred = model.fit(X_train_interpolated, y_train, fit_hyper_parameters = True)[2]\n",
    "x1_error, x2_error = ( (X_pred - X_train_complete)**2.0 ).mean(0)\n",
    "print(\"X_error = {0:}, {1:}\".format(x1_error, x2_error))\n",
    "y_pred = model.predict(X_test)[0]\n",
    "print(\"y_error = {:}\".format( ((y_pred - y_test)**2.0).mean() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
